\documentclass[12pt]{article}

\usepackage{sbc-template}
%\usepackage{pr ogram}
\usepackage{algpseudocode}
\usepackage{algorithm}
%\usepackage{Algorithm2e}
\usepackage{graphicx,url}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{arrows,intersections}
\usepackage[brazil]{babel}   
\usepackage[latin1]{inputenc}  

%\renewcommand{\algorithmiccomment}[1]{\bgroup\hfill//~#1\egroup}
     
\sloppy

\usetikzlibrary{shapes.misc}

\tikzset{cross/.style={cross out, draw=black, minimum size=2*(#1-\pgflinewidth), inner sep=0pt, outer sep=0pt},
%default radius will be 1pt. 
cross/.default={1pt}}


\title{Problema do Máximo Subarray\\ Uma Análise de sua Complexidade}

\author{Matheus Garay Trindade\inst{1}, Guilherme de Freitas Gaiardo\inst{1} }


\address{Departamento de Eletrônica e Computação -- Universidade Federal de Santa Maria\\
  (UFSM)\\
  97.105-900 -- Santa Maria -- RS -- Brazil\\
  \email{\{mtrindade,ggaiardo\}@inf.ufsm.br}
}

\begin{document} 

\maketitle

\begin{abstract}
This paper analises the classic computing problem of the maximum subarray.
Three solutions with different complexities will be presented. 
This complexities will then be proved.
For better understanding of what these complexities actually mean, various 
simulations were conducted with inputs of different sizes.
With so, it is possible to perceive how the algorithm complexity impacts on its
performance.
\end{abstract}
     
\begin{resumo} 

	Este artigo faz uma análise do clássico problema do subarray máximo. Serão apresentadas três soluções com diferentes complexidades. Essas complexidades serão então demonstradas. Para melhor entendimento do que essas complexidades significam, diversas simulações foram feitas com entradas de diferentes tamanhos. Com isso, é possível perceber o quanto a forma de crescimento do algoritmo em função da entrada têm um impacto direto na performance.

\end{resumo}


\section{Definição matemática} \label{sec:firstpage}

As deduções mostradas a seguir são baseadas em \cite{mit_lect_svm}. Para esta técnica, devemos escolher o hiperplano que melhor separa duas classes de dados em um espaço $n$-dimensional. Para ilustrar o ponto de partida, temos a Figura , onde os pontos positivos estão acima do hiperplano e os pontos negativos abaixo.


\begin{figure} [h]
\centering
\begin{tikzpicture}[
    thick,
    >=stealth',
    dot/.style = {
      draw,
      fill = white,
      circle,
      inner sep = 0pt,
      minimum size = 4pt
    }
  ]

  \coordinate (O) at (0,0);
  \draw[->] (-0.3,0) -- (9,0) coordinate[label = {below:$x$}] (xmax);
  
  \draw[->] (0,-0.3) -- (0,7) coordinate[label = {right:$f(x)$}] (ymax);
  \draw (0.8,1.5)  node[fill=blue] {}; 
  \draw (1,1)  node[fill=blue] {}; 
  \draw (0.7,0.8)  node[fill=blue] {};
 

    \draw (3,4)  node[fill=green] {}; 
  \draw (3.3,4.5)  node[fill=green] {}; 
  \draw (2.7,3.7)  node[fill=green] {};
  
    \draw (2.2,3.4)  node[fill=red, label=right:$\vec{d}$] {};
	
	 \draw[dashed, ->] (-0,0) -- (2.2,3.4) coordinate[] ();  
	 
	 
	 
 \draw[dashed] (-0.3,4.4) -- (4.9,-0.3) coordinate[] ();
 
  \draw[->] (-0.0,0) -- (2, 2.2) coordinate[label = below:$\vec{w}$] ();
 
  \draw[-] (-0.3,6.3) -- (7.3,-0.3) coordinate[] ();
  \draw[-] (-0.3,2.5) -- (2.78,-0.3) coordinate[] ();
  
  
 \end{tikzpicture}
	\caption{Dados de Exemplo}
    \label{dataset}
\end{figure}



Para encontrar o hiperplano de fato, é mais conveniente começar por definir o classificador. Para isso, define-se uma função que, dependendo do seu resultado, mostrará se o dado é de uma classe ou de outra, e.g., positivo ou negativo. Assumindo que se conhece hiperplano, pega-se um vetor $\vec{w}$  perpendicular à ele. Temos que a projeção $P$ de um dado $\vec{d}$ sobre esse vetor tem valor dado pela seguinte função:

\begin{center}
\begin{equation}\label{eq1}
P(\vec{d}) = \vec{d} \cdot \vec{w} = \|\vec{d}\|.\|\vec{w}\|.\cos(\theta) = C.\|\vec{d}\|.\cos(\theta)
\end{equation}
\end{center}

Onde $\theta$ é o ângulo entre $\vec{d}$ e $\vec{w}$.  Como a Equação \ref{eq1} mostra, temos em $\|\vec{d}\|.\cos(\theta)$ o comprimeto projeção de um dado sobre o vetor $\vec{w}$.  Caso 
essa projeção seja maior que uma constante $c$, fica evidente que o dado está acima do hiperplano, sendo positivo, se for menor, está abaixo, sendo negativo, e se for igual a essa constante, é ponto do hiperplano. Formalmente, temos:

\begin{center}
\begin{equation}
f(n) =
\begin{cases}
\vec{w}.\vec{d} > c, & \text{se }\vec{d}\text{ é positivo} \\
\vec{w}.\vec{d} < c & \text{se }\vec{d}\text{ é negativo} \\
\vec{w}.\vec{d} = c & \text{se }\vec{d}\text{ está no hiperplano} 
\end{cases}
\end{equation}
\end{center}

Ou ainda:

\begin{center}
\begin{equation}
f(n) =
\begin{cases}
\vec{w}.\vec{d} + b > 0, & \text{se }\vec{d}\text{ é positivo} \\
\vec{w}.\vec{d} + b < 0 & \text{se }\vec{d}\text{ é negativo} \\
\vec{w}.\vec{d} + b = 0 & \text{se }\vec{d}\text{ está no hiperplano} 
\end{cases}
\end{equation}
\end{center}

A seguir, deve-se definir algumas condições. O classificador terá resultado 1 para vetores de suporte positivo e -1 para vetores de suporte negativo:

\begin{center}
\begin{equation}
f(n) =
\begin{cases}
\vec{w}.\vec{d} + b = 1, & \text{se }\vec{d}\text{ é positivo e é vetore de suporte} \\
\vec{w}.\vec{d} + b = -1 & \text{se }\vec{d}\text{ é negativo e é vetore de suporte} \\
\end{cases}
\end{equation}
\end{center}

Assim, podemos então maximizar essa função de forma a encontrar o hiperplano que maximiza
a distância entre as classes. Para isso, como se tem um função com restrições, para maximizá-la, usa-se multiplicadores de Lagrange. Assim, encontra-se o classificador ótimo. Vale que ressaltar que usa-se uma solução númerica para encontrá-lo, mas como o espaço do problema é convexo, ela sempre converge para o classificador ótimo.


\section{SVM Multiclasses} \label{sec:firstpage}

SVM é intrinsicamente uma técnica para classificação binária, entretanto pode facilmente ser convertida para classificar um dado entre mais de duas classes. A seguir, serão apresentadas técnicas usadas para obter essa funcionalidade  \cite{991427}. Para as sessões seguintes, os dados a serem usados são mostrados na Figura \ref{dataset}.


\begin{figure} [h]
\centering
\begin{tikzpicture}[
    thick,
    >=stealth',
    dot/.style = {
      draw,
      fill = white,
      circle,
      inner sep = 0pt,
      minimum size = 4pt
    }
  ]

  \coordinate (O) at (0,0);
  \draw[->] (-0.3,0) -- (8,0) coordinate[label = {below:$x$}] (xmax);
  
  \draw[->] (0,-0.3) -- (0,5) coordinate[label = {right:$f(x)$}] (ymax);
  \draw (0.8,1.5)  node[fill=blue] {}; 
  \draw (1,1)  node[fill=blue] {}; 
  \draw (0.7,0.8)  node[fill=blue] {};
  
  \draw (6.2,1.5)  node[fill=red] {}; 
  \draw (5.9,1)  node[fill=red] {}; 
  \draw (6,0.8)  node[fill=red] {};
  
    \draw (3,4)  node[fill=green] {}; 
  \draw (3.3,4.5)  node[fill=green] {}; 
  \draw (2.7,3.7)  node[fill=green] {};
 \end{tikzpicture}
	\caption{Dados de Exemplo}
    \label{dataset}
\end{figure}




\subsection{One-vs-All}

Essa técnica consiste em criar uma SVM para cada classe separando-a das outras. Ou seja, se o problema for constituído de 3 classes, 3 SVMs serão construídas: uma que separa Azul de Vermelho e Verde, uma Vermelho separado de Azul e Verde e outra que separa Verde de Azul e Vermellho. Após isso, entramos com um dado nessas 3 SVMs e avaliamos o resultado em cada.  

Idealmente, apenas uma SVM classificará o dado como sendo de apenas uma classe, e essa será adotada como classificação final. Por exemplo, apenas a SVM que separa Verde de Azul e Vermelho classificou como sendo de uma classe, nesse caso Verde, sendo essa a classe final. No caso de confusão, i.e., mais de uma classificando como sendo de uma classe só, pode-se usar como critério de desempate a distância para o hiperplano em cada SVM empatada. Um exemplo de Verde contra Azul e Vermelho é mostrada na Figura \ref{svmonevsall}



\begin{figure} [h]
\centering
\begin{tikzpicture}[
    thick,
    >=stealth',
    dot/.style = {
      draw,
      fill = white,
      circle,
      inner sep = 0pt,
      minimum size = 4pt
    }
  ]

  \coordinate (O) at (0,0);
  \draw[->] (-0.3,0) -- (8,0) coordinate[label = {below:$x$}] (xmax);
  
  \draw[->] (0,-0.3) -- (0,5) coordinate[label = {right:$f(x)$}] (ymax);
  \draw (0.8,1.5)  node[fill=gray] {}; 
  \draw (1,1)  node[fill=gray] {}; 
  \draw (0.7,0.8)  node[fill=gray] {};
  
  \draw (6.2,1.5)  node[fill=gray] {}; 
  \draw (5.9,1)  node[fill=gray] {}; 
  \draw (6,0.8)  node[fill=gray] {};
  
    \draw (3,4)  node[fill=green] {}; 
  \draw (3.3,4.5)  node[fill=green] {}; 
  \draw (2.7,3.7)  node[fill=green] {};
  
    \draw[-] (-0.3,2.5) -- (8,2.5) coordinate[] ();
 \end{tikzpicture}
	\caption{Uma das SVM da técnica One-vs-All}
    \label{svmonevsall}
\end{figure}


\subsection{One-vs-One}

Para essa técnica, as classes serão combinadas duas as duas. Ou seja, serão geradas a seguinte quantidade de combinações:

\begin{center}
\begin{equation}
C*\frac{(C-1)}{2}
\end{equation}
\end{center}

Onde $C$ é o número de classes do problema. Assim, no caso exemplo, serão geradas 6 partições de dados. Para cada uma dessas partições será gerada uma SVM. Um exemplo é mostrado na Figura \ref{svmonevsone}.



\begin{figure}[h]
\centering


\begin{tikzpicture}[
    thick,
    >=stealth',
    dot/.style = {
      draw,
      fill = white,
      circle,
      inner sep = 0pt,
      minimum size = 4pt
    }
  ]

  \coordinate (O) at (0,0);
  \draw[->] (-0.3,0) -- (8,0) coordinate[label = {below:$x$}] (xmax);
  
  \draw[->] (0,-0.3) -- (0,5) coordinate[label = {right:$f(x)$}] (ymax);
  \draw (0.8,1.5)  node[fill=gray] {}; 
  \draw (1,1)  node[fill=gray] {}; 
  \draw (0.7,0.8)  node[fill=gray] {};
  
  \draw (6.2,1.5)  node[fill=red] {}; 
  \draw (5.9,1)  node[fill=red] {}; 
  \draw (6,0.8)  node[fill=red] {};
  
    \draw (3,4)  node[fill=green] {}; 
  \draw (3.3,4.4)  node[fill=green] {}; 
  \draw (2.7,3.7)  node[fill=green] {};
  
%   \draw[-] (-0.3,3.5) -- (6.5,-0.3) coordinate[] ();
  
   \draw[-] (1.8,-0.3) -- (6.5,5) coordinate[] ();
 \end{tikzpicture}
	\caption{Uma das SVM da técnica One-vs-One (Verde-vs-Vermelho, Azul é descartado)}
    \label{svmonevsone}
    \end{figure}


Após isso, avaliamos o resultado de cada SVM e faz-se uma votação. Suponha uma SVM que separe a classe $i$ da classe $j$. Se essa SVM classificar como $i$, soma-se um voto para $i$, caso contrário soma-se um voto para $j$. Ao final, avalia-se qual classe recebeu mais votos. A que tiver vencido a votação será o resultado.

Vale ressaltar um ponto, por mais que essa técnica gere mais SVMs do que a técnica One-vs-All, ela ainda pode ser mais rápida de treinar. Isso deve-se ao fato de que cada SVM possui um conjunto de dados de treinamento menor na técnica One-vs-One do que na One-vs-All, o que pode compensar o fato de possuir mais SVMs a serem treinadas em geral

\subsection{Error Correcting Output Code}

Error Correcting Output Code (ECOC) tenta reduzir o problema de um dos vários classificadres não treinar muito bem e introduzir erro no resultado. Para isso, cada classe recebe um código binário conforme mostrado na Tabela \ref{tabelaCodigo}.

\begin{table}[H]  

\centering

\begin{tabular}{ l | c  c c c c c }
  Classe & $f_1$ & $f_2$ & $f_3$& $f_4$& $f_5$\\
  \hline   \hline			
  Vermelho & 1 & 0 & 1 & 0 & 1\\
  Azul & 0 & 0 & 1 & 1 & 1\\
  Verde & 1 & 1 & 0 & 0 & 0\\
  \hline  
\end{tabular}

 \caption{Códigos para a técnica ECOC}
 \label{tabelaCodigo}
\end{table}

A ideia por trás dessa técnica é, dado um dado de entrada, encontrar o seu código e classificá-lo como sendo a que possui o código mais próximo. Para isso, uma SVM binária será criada para cada coluna. Todos os elementos cuja coluna está marcada como 1 definirão uma classe e todos marcados com 0 serão a outra classe. Classifica-se um dado novo sobre essa SVM e descobre-se se essa coluna é 0 ou 1 para ele. Fazendo isso sobre todas as colunas encontra-se o valor do código do dado, bastando então classificando-o como sendo da classe do código mais próximo.

Note que cada classe possui no minimo dois bits de diferença para todas as outras . Isso garante um certo grau de tolerância a possíveis erros em classificadors de uma coluna. Essa tolerância pode ser melhorada aumentando o número de colunas (até certo ponto) a custo de processamento devido ao número de SVMs extras para calcular essas colunas.



\begin{figure}[h]
\centering


\begin{tikzpicture}[
    thick,
    >=stealth',
    dot/.style = {
      draw,
      fill = white,
      circle,
      inner sep = 0pt,
      minimum size = 4pt
    }
  ]

  \coordinate (O) at (0,0);
  \draw[->] (-0.3,0) -- (8,0) coordinate[label = {below:$x$}] (xmax);
  
  \draw[->] (0,-0.3) -- (0,5) coordinate[label = {right:$f(x)$}] (ymax);
  \draw (0.8,1.5)  node[fill=blue] {}; 
  \draw (1,1)  node[fill=blue] {}; 
  \draw (0.7,0.8)  node[fill=blue] {};
  
  \draw (6.2,1.5)  node[fill=gray] {}; 
  \draw (5.9,1)  node[fill=gray] {}; 
  \draw (6,0.8)  node[fill=gray] {};
  
    \draw (3,4)  node[fill=gray] {}; 
  \draw (3.3,4.4)  node[fill=gray] {}; 
  \draw (2.7,3.7)  node[fill=gray] {};
  

   \draw[-] (-0.3,4) -- (5,-0.3) coordinate[] ();
  

 \end{tikzpicture}
	\caption{SVM da primeira coluna da Tabela \ref{tabelaCodigo}}
    \label{svmonevsone}
    \end{figure}



\bibliographystyle{sbc}
\bibliography{sbc-template}%

\end{document}
